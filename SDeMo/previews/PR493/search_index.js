var documenterSearchIndex = {"docs":
[{"location":"explanations/#Explanations","page":"Explanations","title":"Explanations","text":"","category":"section"},{"location":"explanations/#Shapley-values","page":"Explanations","title":"Shapley values","text":"","category":"section"},{"location":"explanations/#Counterfactuals","page":"Explanations","title":"Counterfactuals","text":"","category":"section"},{"location":"explanations/#Partial-responses","page":"Explanations","title":"Partial responses","text":"","category":"section"},{"location":"explanations/#SDeMo.explain","page":"Explanations","title":"SDeMo.explain","text":"explain(model::AbstractSDM, j; observation = nothing, instances = nothing, samples = 100, kwargs..., )\n\nUses the MCMC approximation of Shapley values to provide explanations to specific predictions. The second argument j is the variable for which the explanation should be provided.\n\nThe observation keywords is a row in the instances dataset for which explanations must be provided. If instances is nothing, the explanations will be given on the training data.\n\nAll other keyword arguments are passed to predict.\n\n\n\n\n\n","category":"function"},{"location":"explanations/#SDeMo.counterfactual","page":"Explanations","title":"SDeMo.counterfactual","text":"counterfactual(model::AbstractSDM, x::Vector{T}, yhat, λ; maxiter=100, minvar=5e-5, kwargs...) where {T <: Number}\n\nGenerates one counterfactual explanation given an input vector x, and a target rule to reach yhat. The learning rate is λ. The maximum number of iterations used in the Nelder-Mead algorithm is maxiter, and the variance improvement under which the model will stop is minvar. Other keywords are passed to predict.\n\n\n\n\n\n","category":"function"},{"location":"explanations/#SDeMo.partialresponse","page":"Explanations","title":"SDeMo.partialresponse","text":"partialresponse(model::T, i::Integer, args...; inflated::Bool, kwargs...)\n\nThis method returns the partial response of applying the trained model to a simulated dataset where all variables except i are set to their mean value. The inflated keywork, when set to true, will instead pick a random value within the range of the observations.\n\nThe different arguments that can follow the variable position are\n\nnothing, where the unique values for the i-th variable are used (sorted)\na number, in which point that many evenly spaced points within the range of the variable are used\nan array, in which case each value of this array is evaluated\n\nAll keyword arguments are passed to predict.\n\n\n\n\n\npartialresponse(model::T, i::Integer, j::Integer, s::Tuple=(50, 50); inflated::Bool, kwargs...)\n\nThis method returns the partial response of applying the trained model to a simulated dataset where all variables except i and j are set to their mean value.\n\nThis function will return a grid corresponding to evenly spaced values of i and j, the size of which is given by the last argument s (defaults to 50 × 50).\n\nAll keyword arguments are passed to predict.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo","page":"SDeMo","title":"SDeMo","text":"","category":"section"},{"location":"#SDeMo.__classsplit-Tuple{Any}","page":"SDeMo","title":"SDeMo.__classsplit","text":"__classsplit(y)\n\nReturns a tuple with the presences indices, and the absences indices - this is used to maintain class balance in cross-validation and bagging\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo._explain_many_instances-NTuple{5, Any}","page":"SDeMo","title":"SDeMo._explain_many_instances","text":"_explain_many_instances(f, Z, X, j, n)\n\nApplies explainone_instance on the matrix Z\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo._explain_one_instance-NTuple{5, Any}","page":"SDeMo","title":"SDeMo._explain_one_instance","text":"_explain_one_instance(f, instance, X, j, n)\n\nThis method returns the explanation for the instance at variable j, based on training data X. This is the most granular version of the Shapley values algorithm.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo._mcsample-Tuple{Vector{<:AbstractFloat}, Matrix{<:AbstractFloat}, Int64, Int64}","page":"SDeMo","title":"SDeMo._mcsample","text":"_mcsample(x::Vector{T}, X::Matrix{T}, j::Int64, n::Int64) where {T <:Number}\n\nThis generates a Monte-Carlo sample for Shapley values. The arguments are, in order\n\nx: a single instance (as a vector) to explain\n\nX: a matrix of training data providing the samples for explanation\n\nj: the index of the variable to explain\n\nn: the number of samples to generate for evaluation\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo._validate_one_model!-Union{Tuple{T}, Tuple{T, Any, Any, Vararg{Any}}} where T<:AbstractSDM","page":"SDeMo","title":"SDeMo._validate_one_model!","text":"_validate_one_model!(model::AbstractSDM, fold, τ, kwargs...)\n\nTrains the model and returns the Cv and Ct conf matr. Used internally by cross-validation.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.accuracy","page":"SDeMo","title":"SDeMo.accuracy","text":"accuracy(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of accuracy using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.accuracy-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.accuracy","text":"accuracy(M::ConfusionMatrix)\n\nAccuracy\n\nfracTP + TNTP + TN + FP + FN\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.backwardselection!-Tuple{Any, Any, Any}","page":"SDeMo","title":"SDeMo.backwardselection!","text":"backwardselection!(model, folds, pool; verbose::Bool = false, optimality=mcc, kwargs...)\n\nRemoves variables one at a time until the optimality measure stops increasing. Variables included in pool are not removed.\n\nAll keyword arguments are passed to crossvalidate and train!.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.backwardselection!-Tuple{Any, Any}","page":"SDeMo","title":"SDeMo.backwardselection!","text":"backwardselection!(model, folds; verbose::Bool = false, optimality=mcc, kwargs...)\n\nRemoves variables one at a time until the optimality measure stops increasing.\n\nAll keyword arguments are passed to crossvalidate and train!.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.balancedaccuracy","page":"SDeMo","title":"SDeMo.balancedaccuracy","text":"balancedaccuracy(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of balancedaccuracy using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.balancedaccuracy-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.balancedaccuracy","text":"balanced(M::ConfusionMatrix)\n\nBalanced accuracy\n\nfrac12 (TPR + TNR)\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.bootstrap-Tuple{Any, Any}","page":"SDeMo","title":"SDeMo.bootstrap","text":"bootstrap(y, X; n = 50)\n\nGenerate a series of n bootstrap samples for molde bagging. The present and absent classes are boostrapped separately so that in and out of bag respect (on average) class balance.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.bootstrap-Tuple{SDM}","page":"SDeMo","title":"SDeMo.bootstrap","text":"bootstrap(sdm::SDM; kwargs...)\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.calibrate-Tuple{Type{IsotonicCalibration}, Vector{<:Real}, Vector{Bool}}","page":"SDeMo","title":"SDeMo.calibrate","text":"calibrate(::Type{IsotonicCalibration}, sdm::T; bins=25, kwargs...)\n\nReturns the isotonic calibration result for a given SDM. Isotonic regression is done using the bin membership as a weight, which gives less importance to bins with a lower membership.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.calibrate-Tuple{Type{PlattCalibration}, Vector{<:Real}, Vector{Bool}}","page":"SDeMo","title":"SDeMo.calibrate","text":"calibration(sdm::T; kwargs...)\n\nReturns a function for model calibration, using Platt scaling, optimized with the Newton method. The returned function can be applied to a model output.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.ci-Tuple{Vector{<:ConfusionMatrix}, Any}","page":"SDeMo","title":"SDeMo.ci","text":"ci(C::Vector{<:ConfusionMatrix}, f)\n\nApplies f to all confusion matrices in the vector, and returns the 95% CI.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.ci-Tuple{Vector{<:ConfusionMatrix}}","page":"SDeMo","title":"SDeMo.ci","text":"ci(C::Vector{<:ConfusionMatrix})\n\nApplies the MCC (mcc) to all confusion matrices in the vector, and returns the 95% CI.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.classifier-Tuple{Bagging}","page":"SDeMo","title":"SDeMo.classifier","text":"classifier(model::Bagging)\n\nReturns the classifier used by the model that is used as a template for the bagged model\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.classifier-Tuple{SDM}","page":"SDeMo","title":"SDeMo.classifier","text":"classifier(model::SDM)\n\nReturns the classifier used by the model\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.coinflip-Tuple{Bagging}","page":"SDeMo","title":"SDeMo.coinflip","text":"coinflip(ensemble::Bagging)\n\nVersion of coinflip using the training labels for an homogeneous ensemble.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.coinflip-Tuple{SDM}","page":"SDeMo","title":"SDeMo.coinflip","text":"coinflip(sdm::SDM)\n\nVersion of coinflip using the training labels for an SDM.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.coinflip-Tuple{Vector{Bool}}","page":"SDeMo","title":"SDeMo.coinflip","text":"coinflip(labels::Vector{Bool})\n\nReturns the confusion matrix for the no-skill classifier given a vector of labels. Predictions are made at random, with each class being selected with a probability of one half.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.constantnegative-Tuple{Bagging}","page":"SDeMo","title":"SDeMo.constantnegative","text":"constantnegative(ensemble::Bagging)\n\nVersion of constantnegative using the training labels for an homogeneous ensemble.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.constantnegative-Tuple{SDM}","page":"SDeMo","title":"SDeMo.constantnegative","text":"constantnegative(sdm::SDM)\n\nVersion of constantnegative using the training labels for an SDM.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.constantnegative-Tuple{Vector{Bool}}","page":"SDeMo","title":"SDeMo.constantnegative","text":"constantnegative(labels::Vector{Bool})\n\nReturns the confusion matrix for the constant positive classifier given a vector of labels. Predictions are assumed to always be negative.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.constantpositive-Tuple{Bagging}","page":"SDeMo","title":"SDeMo.constantpositive","text":"constantpositive(ensemble::Bagging)\n\nVersion of constantpositive using the training labels for an homogeneous ensemble.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.constantpositive-Tuple{SDM}","page":"SDeMo","title":"SDeMo.constantpositive","text":"constantpositive(sdm::SDM)\n\nVersion of constantpositive using the training labels for an SDM.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.constantpositive-Tuple{Vector{Bool}}","page":"SDeMo","title":"SDeMo.constantpositive","text":"constantpositive(labels::Vector{Bool})\n\nReturns the confusion matrix for the constant positive classifier given a vector of labels. Predictions are assumed to always be positive.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.correct-Tuple{AbstractCalibration}","page":"SDeMo","title":"SDeMo.correct","text":"correct(cal::AbstractCalibration)\n\nReturns a function that gives a probability given a calibration result.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.correct-Tuple{Vector{<:AbstractCalibration}}","page":"SDeMo","title":"SDeMo.correct","text":"correct(cal::Vector{<:AbstractCalibration})\n\nReturns a function that gives the average of probabilities from a vector of calibration results. This is used when bootstrapping or cross-validating probabilities using a pre-trained model.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.counterfactual-Union{Tuple{T}, Tuple{AbstractSDM, Vector{T}, Any, Any}} where T<:Number","page":"SDeMo","title":"SDeMo.counterfactual","text":"counterfactual(model::AbstractSDM, x::Vector{T}, yhat, λ; maxiter=100, minvar=5e-5, kwargs...) where {T <: Number}\n\nGenerates one counterfactual explanation given an input vector x, and a target rule to reach yhat. The learning rate is λ. The maximum number of iterations used in the Nelder-Mead algorithm is maxiter, and the variance improvement under which the model will stop is minvar. Other keywords are passed to predict.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.crossvalidate-Union{Tuple{T}, Tuple{T, Any}} where T<:AbstractSDM","page":"SDeMo","title":"SDeMo.crossvalidate","text":"crossvalidate(sdm, folds; thr = nothing, kwargs...)\n\nPerforms cross-validation on a model, given a vector of tuples representing the data splits. The threshold can be fixed through the thr keyword arguments. All other keywords are passed to the train! method.\n\nThis method returns two vectors of ConfusionMatrix, with the confusion matrix for each set of validation data first, and the confusion matrix for the training data second.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.crossvalidate-Union{Tuple{T}, Tuple{T, Vararg{Any}}} where T<:AbstractSDM","page":"SDeMo","title":"SDeMo.crossvalidate","text":"crossvalidate(sdm::T, args...; kwargs...) where {T <: AbstractSDM}\n\nPerforms cross-validation using 10-fold validation as a default. Called when crossvalidate is used without a folds second argument.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.dor","page":"SDeMo","title":"SDeMo.dor","text":"dor(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of dor using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.dor-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.dor","text":"dor(M::ConfusionMatrix)\n\nDiagnostic odd ratio, defined as plr/nlr. A useful test has a value larger than unity, and this value has no upper bound.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.explain-Union{Tuple{T}, Tuple{T, Any}} where T<:AbstractSDM","page":"SDeMo","title":"SDeMo.explain","text":"explain(model::AbstractSDM, j; observation = nothing, instances = nothing, samples = 100, kwargs..., )\n\nUses the MCMC approximation of Shapley values to provide explanations to specific predictions. The second argument j is the variable for which the explanation should be provided.\n\nThe observation keywords is a row in the instances dataset for which explanations must be provided. If instances is nothing, the explanations will be given on the training data.\n\nAll other keyword arguments are passed to predict.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.f1","page":"SDeMo","title":"SDeMo.f1","text":"f1(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of f1 using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.f1-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.f1","text":"f1(M::ConfusionMatrix)\n\nF₁ score, defined as the harmonic mean between precision and recall:\n\n2timesfracPPVtimes TPRPPV + TPR\n\nThis uses the more general fscore internally.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.fdir","page":"SDeMo","title":"SDeMo.fdir","text":"fdir(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of fdir using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.fdir-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.fdir","text":"fdir(M::ConfusionMatrix)\n\nFalse discovery rate, 1 - ppv\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.features-Tuple{SDM, Any}","page":"SDeMo","title":"SDeMo.features","text":"features(sdm::SDM, n)\n\nReturns the n-th feature stored in the field X of the SDM.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.features-Tuple{SDM}","page":"SDeMo","title":"SDeMo.features","text":"features(sdm::SDM)\n\nReturns the features stored in the field X of the SDM. Note that the features are an array, and this does not return a copy of it – any change made to the output of this function will change the content of the SDM features.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.fnr","page":"SDeMo","title":"SDeMo.fnr","text":"fnr(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of fnr using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.fnr-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.fnr","text":"fnr(M::ConfusionMatrix)\n\nFalse-negative rate\n\nfracFNFN+TP\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.fomr","page":"SDeMo","title":"SDeMo.fomr","text":"fomr(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of fomr using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.fomr-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.fomr","text":"fomr(M::ConfusionMatrix)\n\nFalse omission rate, 1 - npv\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.forwardselection!-Tuple{Any, Any, Any}","page":"SDeMo","title":"SDeMo.forwardselection!","text":"forwardselection!(model, folds, pool; verbose::Bool = false, optimality=mcc, kwargs...)\n\nAdds variables one at a time until the optimality measure stops increasing. The variables in pool are added at the start.\n\nAll keyword arguments are passed to crossvalidate and train!.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.forwardselection!-Tuple{Any, Any}","page":"SDeMo","title":"SDeMo.forwardselection!","text":"forwardselection!(model, folds; verbose::Bool = false, optimality=mcc, kwargs...)\n\nAdds variables one at a time until the optimality measure stops increasing.\n\nAll keyword arguments are passed to crossvalidate and train!.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.fpr","page":"SDeMo","title":"SDeMo.fpr","text":"fpr(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of fpr using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.fpr-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.fpr","text":"fpr(M::ConfusionMatrix)\n\nFalse-positive rate\n\nfracFPFP+TN\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.fscore","page":"SDeMo","title":"SDeMo.fscore","text":"fscore(M::ConfusionMatrix, β=1.0)\n\nFᵦ score, defined as the harmonic mean between precision and recall, using a positive factor β indicating the relative importance of recall over precision:\n\n(1 + beta^2)timesfracPPVtimes TPR(beta^2 times PPV) + TPR\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.fscore-2","page":"SDeMo","title":"SDeMo.fscore","text":"fscore(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of fscore using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.fscore-Tuple{Real}","page":"SDeMo","title":"SDeMo.fscore","text":"fscore(β::Real)\n\nCreates a function for the Fᵦ score, which takes a confusion matrix as an input.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.gmean-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.gmean","text":"gmean(M::ConfusionMatrix)\n\nGeometric mean of sensitivity and specificity.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.holdout-Tuple{Any, Any}","page":"SDeMo","title":"SDeMo.holdout","text":"holdout(y, X; proportion = 0.2, permute = true)\n\nSets aside a proportion (given by the proportion keyword, defaults to 0.2) of observations to use for validation, and the rest for training. An additional argument permute (defaults to true) can be used to shuffle the order of observations before they are split.\n\nThis method returns a single tuple with the training data first and the validation data second. To use this with crossvalidate, it must be put in [].\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.holdout-Tuple{Bagging, Vararg{Any}}","page":"SDeMo","title":"SDeMo.holdout","text":"holdout(sdm::Bagging)\n\nVersion of holdout using the instances and labels of a bagged SDM. In this case, the instances of the model used as a reference to build the bagged model are used.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.holdout-Tuple{SDM, Vararg{Any}}","page":"SDeMo","title":"SDeMo.holdout","text":"holdout(sdm::SDM)\n\nVersion of holdout using the instances and labels of an SDM.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.hyperparameters!-Union{Tuple{T}, Tuple{T, Symbol, Any}} where T<:(Union{var\"#s12\", var\"#s11\", var\"#s10\"} where {var\"#s12\"<:Classifier, var\"#s11\"<:Transformer, var\"#s10\"<:AbstractSDM})","page":"SDeMo","title":"SDeMo.hyperparameters!","text":"hyperparameters!(tr::HasHyperParams, hp::Symbol, val)\n\nSets the hyper-parameters for a transformer or a classifier\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.hyperparameters-Tuple{Type{<:Union{var\"#s12\", var\"#s11\", var\"#s10\"} where {var\"#s12\"<:Classifier, var\"#s11\"<:Transformer, var\"#s10\"<:AbstractSDM}}}","page":"SDeMo","title":"SDeMo.hyperparameters","text":"hyperparameters(::Type{<:HasHyperParams}) = nothing\n\nReturns the hyper-parameters for a type of classifier or transformer\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.hyperparameters-Tuple{T} where T<:(Union{var\"#s12\", var\"#s11\", var\"#s10\"} where {var\"#s12\"<:Classifier, var\"#s11\"<:Transformer, var\"#s10\"<:AbstractSDM})","page":"SDeMo","title":"SDeMo.hyperparameters","text":"hyperparameters(::HasHyperParams)\n\nReturns the hyper-parameters for a classifier or a transformer\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.hyperparameters-Union{Tuple{T}, Tuple{T, Symbol}} where T<:(Union{var\"#s12\", var\"#s11\", var\"#s10\"} where {var\"#s12\"<:Classifier, var\"#s11\"<:Transformer, var\"#s10\"<:AbstractSDM})","page":"SDeMo","title":"SDeMo.hyperparameters","text":"hyperparameters(::HasHyperParams, ::Symbol)\n\nReturns the value for an hyper-parameter\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.instance-Tuple{SDM, Any}","page":"SDeMo","title":"SDeMo.instance","text":"instance(sdm::SDM, n; strict=true)\n\nReturns the n-th instance stored in the field X of the SDM. If the keyword argument strict is true, only the variables used for prediction are returned.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.iqr","page":"SDeMo","title":"SDeMo.iqr","text":"iqr(x, m=0.25, M=0.75)\n\nReturns the inter-quantile range, by default between 25% and 75% of observations.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.kfold-Tuple{Any, Any}","page":"SDeMo","title":"SDeMo.kfold","text":"kfold(y, X; k = 10, permute = true)\n\nReturns splits of the data in which 1 group is used for validation, and k-1 groups are used for training. All kgroups have the (approximate) same size, and each instance is only used once for validation (andk`-1 times for training). The groups are stratified (so that they have the same prevalence).\n\nThis method returns a vector of tuples, with each entry have the training data first, and the validation data second.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.kfold-Tuple{Bagging, Vararg{Any}}","page":"SDeMo","title":"SDeMo.kfold","text":"kfold(sdm::Bagging)\n\nVersion of kfold using the instances and labels of a bagged SDM. In this case, the instances of the model used as a reference to build the bagged model are used.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.kfold-Tuple{SDM, Vararg{Any}}","page":"SDeMo","title":"SDeMo.kfold","text":"kfold(sdm::SDM)\n\nVersion of kfold using the instances and labels of an SDM.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.labels-Tuple{SDM}","page":"SDeMo","title":"SDeMo.labels","text":"labels(sdm::SDM)\n\nReturns the labels stored in the field y of the SDM – note that this is not a copy of the labels, but the object itself.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.leaveoneout-Tuple{Any, Any}","page":"SDeMo","title":"SDeMo.leaveoneout","text":"leaveoneout(y, X)\n\nReturns the splits for leave-one-out cross-validation. Each sample is used once, on its own, for validation.\n\nThis method returns a vector of tuples, with each entry have the training data first, and the validation data second.\n\nThe rebalanced keyword, defaults to false, drops a random point of the opposite label as per 10.1126/sciadv.adx6976.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.leaveoneout-Tuple{Bagging, Vararg{Any}}","page":"SDeMo","title":"SDeMo.leaveoneout","text":"leaveoneout(sdm::Bagging)\n\nVersion of leaveoneout using the instances and labels of a bagged SDM. In this case, the instances of the model used as a reference to build the bagged model are used.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.leaveoneout-Tuple{SDM, Vararg{Any}}","page":"SDeMo","title":"SDeMo.leaveoneout","text":"leaveoneout(sdm::SDM)\n\nVersion of leaveoneout using the instances and labels of an SDM.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.loadsdm-Tuple{String}","page":"SDeMo","title":"SDeMo.loadsdm","text":"loadsdm(file::String; kwargs...)\n\nLoads a model to a JSON file. The keyword arguments are passed to train!. The model is trained in full upon loading.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.markedness","page":"SDeMo","title":"SDeMo.markedness","text":"markedness(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of markedness using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.markedness-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.markedness","text":"markedness(M::ConfusionMatrix)\n\nMarkedness, a measure similar to informedness (TSS) that emphasizes negative predictions\n\nPPV + NPV -1\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.mcc","page":"SDeMo","title":"SDeMo.mcc","text":"mcc(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of mcc using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.mcc-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.mcc","text":"mcc(M::ConfusionMatrix)\n\nMatthew's correlation coefficient. This is the default measure of model performance, and there are rarely good reasons to use anything else to decide which model to use.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.montecarlo-Tuple{Any, Any}","page":"SDeMo","title":"SDeMo.montecarlo","text":"montecarlo(y, X; n = 100, kwargs...)\n\nReturns n (def. 100) samples of holdout. Other keyword arguments are passed to holdout.\n\nThis method returns a vector of tuples, with each entry have the training data first, and the validation data second.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.montecarlo-Tuple{Bagging, Vararg{Any}}","page":"SDeMo","title":"SDeMo.montecarlo","text":"montecarlo(sdm::Bagging)\n\nVersion of montecarlo using the instances and labels of a bagged SDM. In this case, the instances of the model used as a reference to build the bagged model are used.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.montecarlo-Tuple{SDM, Vararg{Any}}","page":"SDeMo","title":"SDeMo.montecarlo","text":"montecarlo(sdm::SDM)\n\nVersion of montecarlo using the instances and labels of an SDM.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.nlr","page":"SDeMo","title":"SDeMo.nlr","text":"nlr(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of nlr using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.nlr-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.nlr","text":"nlr(M::ConfusionMatrix)\n\nNegative likelihood ratio\n\nfracFNRTNR\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.noselection!-Tuple{Any, Any}","page":"SDeMo","title":"SDeMo.noselection!","text":"noselection!(model, folds; verbose::Bool = false, kwargs...)\n\nReturns the model to the state where all variables are used.\n\nAll keyword arguments are passed to train!.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.noselection!-Tuple{Any}","page":"SDeMo","title":"SDeMo.noselection!","text":"noselection!(model; verbose::Bool = false, kwargs...)\n\nReturns the model to the state where all variables are used.\n\nAll keyword arguments are passed to train!. For convenience, this version does not require a folds argument, as it would be unused anyway.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.noskill-Tuple{Bagging}","page":"SDeMo","title":"SDeMo.noskill","text":"noskill(ensemble::Bagging)\n\nVersion of noskill using the training labels for an homogeneous ensemble.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.noskill-Tuple{SDM}","page":"SDeMo","title":"SDeMo.noskill","text":"noskill(sdm::SDM)\n\nVersion of noskill using the training labels for an SDM.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.noskill-Tuple{Vector{Bool}}","page":"SDeMo","title":"SDeMo.noskill","text":"noskill(labels::Vector{Bool})\n\nReturns the confusion matrix for the no-skill classifier given a vector of labels. Predictions are made at random, with each class being selected by its proportion in the training data.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.npv","page":"SDeMo","title":"SDeMo.npv","text":"npv(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of npv using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.npv-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.npv","text":"npv(M::ConfusionMatrix)\n\nNegative predictive value\n\nfracTNTN+FN\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.outofbag-Tuple{Bagging}","page":"SDeMo","title":"SDeMo.outofbag","text":"outofbag(ensemble::Bagging; kwargs...)\n\nThis method returns the confusion matrix associated to the out of bag error, wherein the succes in predicting instance i is calculated on the basis of all models that have not been trained on i. The consensus of the different models is a simple majority rule.\n\nThe additional keywords arguments are passed to predict.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.partialresponse-Union{Tuple{T}, Tuple{T, Integer, Integer}, Tuple{T, Integer, Integer, Tuple}} where T<:AbstractSDM","page":"SDeMo","title":"SDeMo.partialresponse","text":"partialresponse(model::T, i::Integer, j::Integer, s::Tuple=(50, 50); inflated::Bool, kwargs...)\n\nThis method returns the partial response of applying the trained model to a simulated dataset where all variables except i and j are set to their mean value.\n\nThis function will return a grid corresponding to evenly spaced values of i and j, the size of which is given by the last argument s (defaults to 50 × 50).\n\nAll keyword arguments are passed to predict.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.partialresponse-Union{Tuple{T}, Tuple{T, Integer, Vararg{Any}}} where T<:AbstractSDM","page":"SDeMo","title":"SDeMo.partialresponse","text":"partialresponse(model::T, i::Integer, args...; inflated::Bool, kwargs...)\n\nThis method returns the partial response of applying the trained model to a simulated dataset where all variables except i are set to their mean value. The inflated keywork, when set to true, will instead pick a random value within the range of the observations.\n\nThe different arguments that can follow the variable position are\n\nnothing, where the unique values for the i-th variable are used (sorted)\na number, in which point that many evenly spaced points within the range of the variable are used\nan array, in which case each value of this array is evaluated\n\nAll keyword arguments are passed to predict.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.plr","page":"SDeMo","title":"SDeMo.plr","text":"plr(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of plr using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.plr-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.plr","text":"plr(M::ConfusionMatrix)\n\nPositive likelihood ratio\n\nfracTPRFPR\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.ppv","page":"SDeMo","title":"SDeMo.ppv","text":"ppv(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of ppv using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.ppv-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.ppv","text":"ppv(M::ConfusionMatrix)\n\nPositive predictive value\n\nfracTPTP+FP\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.precision","page":"SDeMo","title":"SDeMo.precision","text":"precision(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of precision using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.precision-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.precision","text":"precision(M::ConfusionMatrix)\n\nAlias for ppv, the positive predictive value\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.prune!-Tuple{Any, Any, Any}","page":"SDeMo","title":"SDeMo.prune!","text":"prune!(tree, X, y)\n\nThis function will take each twig in a tree, and merge the one with the worst contribution to information gain.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.recall","page":"SDeMo","title":"SDeMo.recall","text":"recall(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of recall using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.recall-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.recall","text":"recall(M::ConfusionMatrix)\n\nAlias for tpr, the true positive rate\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.reliability-Tuple{AbstractSDM}","page":"SDeMo","title":"SDeMo.reliability","text":"reliability(sdm::AbstractSDM, link::Function=identity; bins=9, kwargs...)\n\nReturns a binned reliability curve for a trained model, where the raw scores are transformed with a specified link function (which defaults to identity). Keyword arguments other than bins are passed to predict. \n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.reliability-Tuple{Vector{<:Real}, Vector{Bool}}","page":"SDeMo","title":"SDeMo.reliability","text":"reliability(yhat, y; bins=9)\n\nReturns a binned reliability curve for a series of predicted quantitative scores and a series of truth values.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.reset!","page":"SDeMo","title":"SDeMo.reset!","text":"reset!(sdm::SDM, thr=0.5)\n\nResets a model, with a potentially specified value of the threshold. This amounts to re-using all the variables, and removing the tuned threshold version.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.sensitivity","page":"SDeMo","title":"SDeMo.sensitivity","text":"sensitivity(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of sensitivity using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.sensitivity-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.sensitivity","text":"sensitivity(M::ConfusionMatrix)\n\nAlias for tpr, the true positive rate\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.specificity","page":"SDeMo","title":"SDeMo.specificity","text":"specificity(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of specificity using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.specificity-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.specificity","text":"specificity(M::ConfusionMatrix)\n\nAlias for tnr, the true negative rate\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.stepwisevif!","page":"SDeMo","title":"SDeMo.stepwisevif!","text":"stepwisevif!(model::SDM, limit, tr=:;kwargs...)\n\nDrops the variables with the largest variance inflation from the model, until all VIFs are under the threshold. The last positional argument (defaults to :) is the indices to use for the VIF calculation. All keyword arguments are passed to train!.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.threshold!-Tuple{SDM, Any}","page":"SDeMo","title":"SDeMo.threshold!","text":"threshold!(sdm::SDM, τ)\n\nSets the value of the threshold.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.threshold!-Tuple{SDM, Vector{Tuple{Vector{Int64}, Vector{Int64}}}}","page":"SDeMo","title":"SDeMo.threshold!","text":"threshold!(sdm::SDM, folds::Vector{Tuple{Vector{Int}, Vector{Int}}}; optimality=mcc)\n\nOptimizes the threshold for a SDM using cross-validation, as given by the folds. This is meant to be used after cross-validation, as it will cross-validate the threshold across all the training data in a way that is a little more robust than the version in train!.\n\nThe specific technique used is to train one model per fold, then aggregate all of their predictions on the validation data, and find the value of the threshold that maximizes the average performance across folds.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.threshold!-Tuple{SDM}","page":"SDeMo","title":"SDeMo.threshold!","text":"threshold!(sdm::SDM; kwargs...)\n\nVersion of threshold! without folds, for which the default of 10-fold validation will be used.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.threshold-Tuple{SDM}","page":"SDeMo","title":"SDeMo.threshold","text":"threshold(sdm::SDM)\n\nThis returns the value above which the score returned by the SDM is considered to be a presence.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.tnr","page":"SDeMo","title":"SDeMo.tnr","text":"tnr(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of tnr using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.tnr-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.tnr","text":"tnr(M::ConfusionMatrix)\n\nTrue-negative rate\n\nfracTNTN+FP\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.tpr","page":"SDeMo","title":"SDeMo.tpr","text":"tpr(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of tpr using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.tpr-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.tpr","text":"tpr(M::ConfusionMatrix)\n\nTrue-positive rate\n\nfracTPTP+FN\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.train!-Tuple{AdaBoost}","page":"SDeMo","title":"SDeMo.train!","text":"train!(b::AdaBoost; kwargs...)\n\nTrains all the model in an ensemble model - the keyword arguments are passed to train! for each model. Note that this also retrains the original model. If the original model contains transformers, they are re-trained for each learner that is added to the ensemble. This is crucial as learners are re-trained on proportionally weighted samples of the training data, and not re-training the transformers would create data leakage.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.train!-Tuple{Bagging}","page":"SDeMo","title":"SDeMo.train!","text":"train!(ensemble::Bagging; kwargs...)\n\nTrains all the models in an ensemble model - the keyword arguments are passed to train! for each model. Note that this retrains the entire model, which includes the transformers.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.train!-Tuple{Ensemble}","page":"SDeMo","title":"SDeMo.train!","text":"train!(ensemble::Ensemble; kwargs...)\n\nTrains all the model in an heterogeneous ensemble model - the keyword arguments are passed to train! for each model. Note that this retrains the entire model, which includes the transformers.\n\nThe keywod arguments are passed to train! and can include the training indices.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.train!-Tuple{SDM}","page":"SDeMo","title":"SDeMo.train!","text":"train!(sdm::SDM; threshold=true, training=:, optimality=mcc)\n\nThis is the main training function to train a SDM.\n\nThe three keyword arguments are:\n\ntraining: defaults to :, and is the range (or alternatively the indices) of the data that are used to train the model\nthreshold: defaults to true, and performs moving threshold by evaluating 200 possible values between the minimum and maximum output of the model, and returning the one that is optimal\noptimality: defaults to mcc, and is the function applied to the confusion matrix to evaluate which value of the threshold is the best\nabsences: defaults to false, and indicates whether the (pseudo) absences are used to train the transformer; when using actual absences, this should be set to true\n\nInternally, this function trains the transformer, then projects the data, then trains the classifier. If threshold is true, the threshold is then optimized.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.transformer-Tuple{Bagging}","page":"SDeMo","title":"SDeMo.transformer","text":"transformer(model::Bagging)\n\nReturns the transformer used by the model that is used as a template for the bagged model\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.transformer-Tuple{SDM}","page":"SDeMo","title":"SDeMo.transformer","text":"transformer(model::SDM)\n\nReturns the transformer used by the model\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.trueskill","page":"SDeMo","title":"SDeMo.trueskill","text":"trueskill(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of trueskill using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.trueskill-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.trueskill","text":"trueskill(M::ConfusionMatrix)\n\nTrue skill statistic (a.k.a Youden's J, or informedness)\n\nTPR + TNR - 1\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.variableimportance-Tuple{Any, Any}","page":"SDeMo","title":"SDeMo.variableimportance","text":"variableimportance(model, folds; kwargs...)\n\nReturns the importance of all variables in the model. The keywords are passed to variableimportance.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.variableimportance-Union{Tuple{T}, Tuple{T, Any, Any}} where T<:AbstractSDM","page":"SDeMo","title":"SDeMo.variableimportance","text":"variableimportance(model, folds, variable; reps=10, optimality=mcc, kwargs...)\n\nReturns the importance of one variable in the model. The samples keyword fixes the number of bootstraps to run (defaults to 10, which is not enough!).\n\nThe keywords are passed to ConfusionMatrix.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.variables!-Tuple{Bagging, Vector{Int64}}","page":"SDeMo","title":"SDeMo.variables!","text":"variables!(ensemble::Bagging, v::Vector{Int})\n\nSets the variable of the top-level model, and then sets the variables of each model in the ensemble.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.variables!-Tuple{SDM, Vector{Int64}}","page":"SDeMo","title":"SDeMo.variables!","text":"variables!(sdm::SDM, v)\n\nSets the list of variables.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.variables!-Union{Tuple{M}, Tuple{T}, Tuple{M, Type{T}, Vector{Tuple{Vector{Int64}, Vector{Int64}}}}} where {T<:VariableSelectionStrategy, M<:Union{Bagging, SDM}}","page":"SDeMo","title":"SDeMo.variables!","text":"variables!(model::AbstractSDM, ::Type{T}, folds::Vector{Tuple{Vector{Int}, Vector{Int}}}; included=Int[], optimality=mcc, verbose::Bool=false, bagfeatures::Bool=false, kwargs...) where {T <: VariableSelectionStrategy}\n\nPerforms variable selection based on a selection strategy, with a possible folds for cross-validation. If omitted, this defaults to k-folds.\n\nThe model is retrained on the optimal set of variables after training.\n\nKeywords:\n\nincluded (Int[]), a list of variables that must be included in the model\noptimality (mcc), the measure to optimise at each round of variable selection\nverbose (false), whether the performance should be returned after each round of variable selection\nbagfeatures (false), whether bagfeatures! should be called on each model in an homogeneous ensemble\nall other keywords are passed to train! and crossvalidate\n\nImportant notes:\n\nWhen using bagfeatures with a pool of included variables, they will always be present in the overall model, but not necessarilly in each model of the ensemble\nWhen using VarianceInflationFactor, the variable selection will stop even if the VIF is above the threshold, if it means producing a model with a lower performance – using variables! will always lead to a better model\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.variables!-Union{Tuple{N}, Tuple{M}, Tuple{M, Type{StrictVarianceInflationFactor{N}}, Vararg{Any}}} where {M<:Union{Bagging, SDM}, N}","page":"SDeMo","title":"SDeMo.variables!","text":"variables!(model::M, ::Type{StrictVarianceInflationFactor{N}}, args...; included::Vector{Int}=Int[], optimality=mcc, verbose::Bool=false, bagfeatures::Bool=false, kwargs...) where {M <: Union{SDM, Bagging}, N}\n\nVersion of the variable selection for the strict VIF case. This may result in a worse model, and for this reason there is no cross-validation.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.variables-Tuple{SDM}","page":"SDeMo","title":"SDeMo.variables","text":"variables(sdm::SDM)\n\nReturns the list of variables used by the SDM – these may be ordered by importance. This does not return a copy of the variables array, but the array itself.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.vif-Union{Tuple{Matrix{T}}, Tuple{T}} where T<:Number","page":"SDeMo","title":"SDeMo.vif","text":"vif(::Matrix)\n\nReturns the variance inflation factor for each variable in a matrix, as the diagonal of the inverse of the correlation matrix between predictors.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.vif-Union{Tuple{T}, Tuple{T, Any}} where T<:AbstractSDM","page":"SDeMo","title":"SDeMo.vif","text":"vif(::AbstractSDM, tr=:)\n\nReturns the VIF for the variables used in a SDM, optionally restricting to some training instances (defaults to : for all points). The VIF is calculated on the de-meaned predictors.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.writesdm-Tuple{String, SDM}","page":"SDeMo","title":"SDeMo.writesdm","text":"writesdm(file::String, model::SDM)\n\nWrites a model to a JSON file. This method is very bare-bones, and only saves the structure of the model, as well as the data.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.κ","page":"SDeMo","title":"SDeMo.κ","text":"κ(C::Vector{<:ConfusionMatrix}, full::Bool=false)\n\nVersion of κ using a vector of confusion matrices. Returns the mean, and when the second argument is true, returns a tuple where the second argument is the CI.\n\n\n\n\n\n","category":"function"},{"location":"#SDeMo.κ-Tuple{ConfusionMatrix}","page":"SDeMo","title":"SDeMo.κ","text":"κ(M::ConfusionMatrix)\n\nCohen's κ\n\n\n\n\n\n","category":"method"},{"location":"#StatsAPI.predict-Tuple{AdaBoost}","page":"SDeMo","title":"StatsAPI.predict","text":"TODO\n\n\n\n\n\n","category":"method"},{"location":"#StatsAPI.predict-Tuple{Bagging}","page":"SDeMo","title":"StatsAPI.predict","text":"StatsAPI.predict(ensemble::Bagging; kwargs...)\n\nPredicts the ensemble model for all training data.\n\n\n\n\n\n","category":"method"},{"location":"#StatsAPI.predict-Tuple{Ensemble}","page":"SDeMo","title":"StatsAPI.predict","text":"StatsAPI.predict(ensemble::Ensemble; kwargs...)\n\nPredicts the heterogeneous ensemble model for all training data.\n\n\n\n\n\n","category":"method"},{"location":"#StatsAPI.predict-Tuple{SDM}","page":"SDeMo","title":"StatsAPI.predict","text":"StatsAPI.predict(sdm::SDM; kwargs...)\n\nThis method performs the prediction on the entire set of training data available for the training of an SDM.\n\n\n\n\n\n","category":"method"},{"location":"#StatsAPI.predict-Union{Tuple{T}, Tuple{AdaBoost, Matrix{T}}} where T<:Number","page":"SDeMo","title":"StatsAPI.predict","text":"predict(::AdaBoost, ::Matrix{T}; kwargs...)\n\nPredicts with a trained AdaBoost model.\n\n\n\n\n\n","category":"method"},{"location":"#StatsAPI.predict-Union{Tuple{T}, Tuple{Bagging, Matrix{T}}} where T<:Number","page":"SDeMo","title":"StatsAPI.predict","text":"StatsAPI.predict(ensemble::Bagging, X; consensus = median, kwargs...)\n\nReturns the prediction for the ensemble of models a dataset X. The function used to aggregate the outputs from different models is consensus (defaults to median). All other keyword arguments are passed to predict.\n\nTo get a direct estimate of the variability, the consensus function can be changed to iqr (inter-quantile range), or any measure of variance.\n\n\n\n\n\n","category":"method"},{"location":"#StatsAPI.predict-Union{Tuple{T}, Tuple{Ensemble, Matrix{T}}} where T<:Number","page":"SDeMo","title":"StatsAPI.predict","text":"StatsAPI.predict(ensemble::Ensemble, X; consensus = median, kwargs...)\n\nReturns the prediction for the heterogeneous ensemble of models a dataset X. The function used to aggregate the outputs from different models is consensus (defaults to median). All other keyword arguments are passed to predict.\n\nTo get a direct estimate of the variability, the consensus function can be changed to iqr (inter-quantile range), or any measure of variance.\n\n\n\n\n\n","category":"method"},{"location":"#StatsAPI.predict-Union{Tuple{T}, Tuple{SDM, Matrix{T}}} where T<:Number","page":"SDeMo","title":"StatsAPI.predict","text":"StatsAPI.predict(sdm::SDM, X; threshold = true)\n\nThis is the main prediction function, and it takes as input an SDM and a matrix of features. The only keyword argument is threshold, which determines whether the prediction is returned raw or as a binary value (default is true).\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.AbstractBoostedSDM","page":"SDeMo","title":"SDeMo.AbstractBoostedSDM","text":"AbstractBoostedSDM\n\nThis type covers model that use boosting to iteratively improve on the least well predicted instances of a problem.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.AbstractEnsembleSDM","page":"SDeMo","title":"SDeMo.AbstractEnsembleSDM","text":"AbstractEnsembleSDM\n\nThis abstract types covers model that combine different SDMs to make a prediction, which currently covers Bagging and Ensemble.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.AbstractSDM","page":"SDeMo","title":"SDeMo.AbstractSDM","text":"AbstractSDM\n\nThis abstract type covers the regular, ensemble, and boosted models.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.AdaBoost","page":"SDeMo","title":"SDeMo.AdaBoost","text":"AdaBoost <: AbstractBoostedSDM\n\nA type for AdaBoost that contains the model, a vector of learners, a vector of learner weights, a number of boosting iterations, and the weights w of each point.\n\nNote that this type uses training by re-sampling data according to their weights, as opposed to re-training on all samples and weighting internally. Some learners may have negative weights, in which case their predictions are flipped at prediction time.\n\nThe learning rate η defaults to one, but can be lowered to improve stability / prevent over-fitting. The learning rate must be larger than 0, and decreasing it should lead to a larger number of iterations.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.AllVariables","page":"SDeMo","title":"SDeMo.AllVariables","text":"AllVariables\n\nAll variables in the training dataset are used. Note that this also crossvalidates and trains the model.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.BIOCLIM","page":"SDeMo","title":"SDeMo.BIOCLIM","text":"BIOCLIM\n\nBIOCLIM\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.BackwardSelection","page":"SDeMo","title":"SDeMo.BackwardSelection","text":"ForwardSelection\n\nVariables are removed one at a time until the performance of the models stops improving.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.Bagging","page":"SDeMo","title":"SDeMo.Bagging","text":"Bagging\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.Bagging-Tuple{SDM, Integer}","page":"SDeMo","title":"SDeMo.Bagging","text":"Bagging(model::SDM, n::Integer)\n\nCreates a bag from SDM\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.Bagging-Tuple{SDM, Vector}","page":"SDeMo","title":"SDeMo.Bagging","text":"Bagging(model::SDM, bags::Vector)\n\nblah\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.ChainedTransform","page":"SDeMo","title":"SDeMo.ChainedTransform","text":"ChainedTransform{T1, T2}\n\nA transformer that applies, in sequence, a pair of other transformers. This can be used to, for example, do a PCA then a z-score on the projected space. This is limited to two steps because the value of chaining more transformers is doubtful. We may add support for more complex transformations in future versions.\n\nThe first and second steps are accessible through first and last.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.Classifier","page":"SDeMo","title":"SDeMo.Classifier","text":"Classifier\n\nThis abstract type covers all algorithms to convert transformed data into prediction.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.ConfusionMatrix","page":"SDeMo","title":"SDeMo.ConfusionMatrix","text":"ConfusionMatrix{T <: Number}\n\nA structure to store the true positives, true negatives, false positives, and false negatives counts (or proportion) during model evaluation. Empty confusion matrices can be created using the zero method.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.ConfusionMatrix-Tuple{Bagging}","page":"SDeMo","title":"SDeMo.ConfusionMatrix","text":"ConfusionMatrix(ensemble::Bagging; kwargs...)\n\nPerforms the predictions for an SDM, and compare to the labels used for training. The keyword arguments are passed to the predict method.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.ConfusionMatrix-Tuple{SDM}","page":"SDeMo","title":"SDeMo.ConfusionMatrix","text":"ConfusionMatrix(sdm::SDM; kwargs...)\n\nPerforms the predictions for an SDM, and compare to the labels used for training. The keyword arguments are passed to the predict method.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.ConfusionMatrix-Tuple{Vector{Bool}, Vector{Bool}}","page":"SDeMo","title":"SDeMo.ConfusionMatrix","text":"ConfusionMatrix(pred::Vector{Bool}, truth::Vector{Bool})\n\nGiven a vector of binary predictions and a vector of ground truths, returns the confusion matrix.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.ConfusionMatrix-Union{Tuple{T}, Tuple{Vector{T}, Vector{Bool}, T}} where T<:Number","page":"SDeMo","title":"SDeMo.ConfusionMatrix","text":"ConfusionMatrix(pred::Vector{T}, truth::Vector{Bool}, τ::T) where {T <: Number}\n\nGiven a vector of scores and a vector of ground truths, as well as a threshold, transforms the score into binary predictions and returns the confusion matrix.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.ConfusionMatrix-Union{Tuple{T}, Tuple{Vector{T}, Vector{Bool}}} where T<:Number","page":"SDeMo","title":"SDeMo.ConfusionMatrix","text":"ConfusionMatrix(pred::Vector{T}, truth::Vector{Bool}) where {T <: Number}\n\nGiven a vector of scores and a vector of truth, returns the confusion matrix under the assumption that the score are probabilities and that the threshold is one half.\n\n\n\n\n\n","category":"method"},{"location":"#SDeMo.DecisionTree","page":"SDeMo","title":"SDeMo.DecisionTree","text":"DecisionTree\n\nThe depth and number of nodes can be adjusted with maxnodes! and maxdepth!.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.Ensemble","page":"SDeMo","title":"SDeMo.Ensemble","text":"Ensemble\n\nAn heterogeneous ensemble model is defined as a vector of SDMs. Bagging models can also be used.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.ForwardSelection","page":"SDeMo","title":"SDeMo.ForwardSelection","text":"ForwardSelection\n\nVariables are included one at a time until the performance of the models stops improving.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.Logistic","page":"SDeMo","title":"SDeMo.Logistic","text":"Logistic\n\nLogistic regression with default learning rate of 0.01, penalization (L2) of 0.1, and 2000 epochs. Note that interaction terms can be turned on and off through the use of the interactions field. Possible values are :all (default), :self (only squared terms), and :none (no interactions).\n\nThe verbose field (defaults to false) can be used to show the progress of gradient descent, by showing the loss every 100 epochs, or to the value of the verbosity field. Note that when doing cross-validation, the loss on the validation data will be automatically reported.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.MultivariateTransform","page":"SDeMo","title":"SDeMo.MultivariateTransform","text":"MultivariateTransform{T} <: Transformer\n\nT is a multivariate transformation, likely offered through the MultivariateStats package. The transformations currently supported are PCA, PPCA, KernelPCA, and Whitening, and they are documented through their type aliases (e.g. PCATransform).\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.NaiveBayes","page":"SDeMo","title":"SDeMo.NaiveBayes","text":"NaiveBayes\n\nNaive Bayes Classifier\n\nBy default, upon training, the prior probability will be set to the prevalence of the training data.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.PCATransform","page":"SDeMo","title":"SDeMo.PCATransform","text":"PCATransform\n\nThe PCA transform will project the model features, which also serves as a way to decrease the dimensionality of the problem. Note that this method will only use the training instances, and unless the absences=true keyword is used, only the present cases. This ensure that there is no data leak (neither validation data nor the data from the raster are used).\n\nThis is an alias for MultivariateTransform{PCA}.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.RawData","page":"SDeMo","title":"SDeMo.RawData","text":"RawData\n\nA transformer that does nothing to the data. This is passing the raw data to the classifier, and can be a good first step for models that assume that the features are independent, or are not sensitive to the scale of the features.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.SDM","page":"SDeMo","title":"SDeMo.SDM","text":"SDM\n\nThis type specifies a full model, which is composed of a transformer (which applies a transformation on the data), a classifier (which returns a quantitative score), a threshold (above which the score corresponds to the prediction of a presence).\n\nIn addition, the SDM carries with it the training features and labels, as well as a vector of indices indicating which variables are actually used by the model.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.StrictVarianceInflationFactor","page":"SDeMo","title":"SDeMo.StrictVarianceInflationFactor","text":"StrictVarianceInflationFactor{N}\n\nRemoves variables one at a time until the largest VIF is lower than N (a floating point number). By contrast with VarianceInflationFactor, this approach to variable selection will not cross-validate the model, and might result in a model that is far worse than any other variable selection technique.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.Transformer","page":"SDeMo","title":"SDeMo.Transformer","text":"Transformer\n\nThis abstract type covers all transformations that are applied to the data before fitting the classifier.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.VariableSelectionStrategy","page":"SDeMo","title":"SDeMo.VariableSelectionStrategy","text":"VariableSelectionStrategy\n\nThis is an abstract type to which all variable selection types belong. The variable selection methods should define a method for variables!, whose first argument is a model, and the second argument is a selection strategy. The third and fourth positional arguments are, respectively, a list of variables to be included, and the folds to use for cross-validation. They can be omitted and would default to no default variables, and k-fold cross-validation.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.VarianceInflationFactor","page":"SDeMo","title":"SDeMo.VarianceInflationFactor","text":"VarianceInflationFactor{N}\n\nRemoves variables one at a time until the largest VIF is lower than N (a floating point number), or the performancde of the model stops increasing. Note that the resulting set of variables may have a largest VIF larger than the threshold. See StrictVarianceInflationFactor for an alternative.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.WhiteningTransform","page":"SDeMo","title":"SDeMo.WhiteningTransform","text":"WhiteningTransform\n\nThe whitening transformation is a linear transformation of the input variables, after which the new variables have unit variance and no correlation. The input is transformed into white noise.\n\nBecause this transform will usually keep the first variable \"as is\", and then apply increasingly important perturbations on the subsequent variables, it is sensitive to the order in which variables are presented, and is less useful when applying tools for interpretation.\n\nThis is an alias for MultivariateTransform{Whitening}.\n\n\n\n\n\n","category":"type"},{"location":"#SDeMo.ZScore","page":"SDeMo","title":"SDeMo.ZScore","text":"ZScore\n\nA transformer that scales and centers the data, using only the data that are avaiable to the model at training time.\n\nFor all variables in the SDM features (regardless of whether they are used), this transformer will store the observed mean and standard deviation. There is no correction on the sample size, because there is no reason to expect that the sample size will be the same for the training and prediction situation.\n\n\n\n\n\n","category":"type"}]
}
